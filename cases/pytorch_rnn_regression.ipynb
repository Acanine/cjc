{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-11T19:51:55.776410",
     "start_time": "2017-08-11T19:51:55.772595"
    }
   },
   "source": [
    "https://github.com/SherlockLiao/pytorch-beginner/blob/master/01-Linear%20Regression/Linear_Regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-11T19:48:51.782822",
     "start_time": "2017-08-11T19:48:51.778322"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-11T19:49:42.346218",
     "start_time": "2017-08-11T19:49:42.177587"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIMNdkhpkuEtSgwx3SWrQ/wFuP2h3Znmi9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dfe1e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168],\n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042],\n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573],\n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827],\n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n",
    "\n",
    "plt.plot(x_train, y_train, 'r.')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-11T22:37:13.620788",
     "start_time": "2017-08-11T22:37:13.611415"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Linear in module torch.nn.modules.linear:\n",
      "\n",
      "class Linear(torch.nn.modules.module.Module)\n",
      " |  Applies a linear transformation to the incoming data: :math:`y = Ax + b`\n",
      " |  \n",
      " |  Args:\n",
      " |      in_features: size of each input sample\n",
      " |      out_features: size of each output sample\n",
      " |      bias: If set to False, the layer will not learn an additive bias. Default: True\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, in\\_features)`\n",
      " |      - Output: :math:`(N, out\\_features)`\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight: the learnable weights of the module of shape (out_features x in_features)\n",
      " |      bias:   the learnable bias of the module of shape (out_features)\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> m = nn.Linear(20, 30)\n",
      " |      >>> input = autograd.Variable(torch.randn(128, 20))\n",
      " |      >>> output = m(input)\n",
      " |      >>> print(output.size())\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Linear\n",
      " |      torch.nn.modules.module.Module\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, in_features, out_features, bias=True)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |  \n",
      " |  cpu(self, device_id=None)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |  \n",
      " |  cuda(self, device_id=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device_id (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. The keys of :attr:`state_dict` must\n",
      " |      exactly match the keys returned by this module's :func:`state_dict()`\n",
      " |      function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, memo=None)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time :func:`forward` computes an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='')\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-11T19:50:32.072194",
     "start_time": "2017-08-11T19:50:32.065883"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # input and output is 1 dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-11T19:50:40.282808",
     "start_time": "2017-08-11T19:50:40.279785"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义loss和优化函数\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-11T19:50:55.885261",
     "start_time": "2017-08-11T19:50:55.527602"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[20/1000], loss: 34.622749\n",
      "Epoch[40/1000], loss: 24.584074\n",
      "Epoch[60/1000], loss: 17.489223\n",
      "Epoch[80/1000], loss: 12.474909\n",
      "Epoch[100/1000], loss: 8.931002\n",
      "Epoch[120/1000], loss: 6.426298\n",
      "Epoch[140/1000], loss: 4.656044\n",
      "Epoch[160/1000], loss: 3.404861\n",
      "Epoch[180/1000], loss: 2.520526\n",
      "Epoch[200/1000], loss: 1.895463\n",
      "Epoch[220/1000], loss: 1.453635\n",
      "Epoch[240/1000], loss: 1.141310\n",
      "Epoch[260/1000], loss: 0.920511\n",
      "Epoch[280/1000], loss: 0.764395\n",
      "Epoch[300/1000], loss: 0.653995\n",
      "Epoch[320/1000], loss: 0.575905\n",
      "Epoch[340/1000], loss: 0.520649\n",
      "Epoch[360/1000], loss: 0.481532\n",
      "Epoch[380/1000], loss: 0.453820\n",
      "Epoch[400/1000], loss: 0.434169\n",
      "Epoch[420/1000], loss: 0.420215\n",
      "Epoch[440/1000], loss: 0.410288\n",
      "Epoch[460/1000], loss: 0.403206\n",
      "Epoch[480/1000], loss: 0.398136\n",
      "Epoch[500/1000], loss: 0.394487\n",
      "Epoch[520/1000], loss: 0.391843\n",
      "Epoch[540/1000], loss: 0.389910\n",
      "Epoch[560/1000], loss: 0.388478\n",
      "Epoch[580/1000], loss: 0.387401\n",
      "Epoch[600/1000], loss: 0.386575\n",
      "Epoch[620/1000], loss: 0.385926\n",
      "Epoch[640/1000], loss: 0.385402\n",
      "Epoch[660/1000], loss: 0.384968\n",
      "Epoch[680/1000], loss: 0.384596\n",
      "Epoch[700/1000], loss: 0.384268\n",
      "Epoch[720/1000], loss: 0.383972\n",
      "Epoch[740/1000], loss: 0.383698\n",
      "Epoch[760/1000], loss: 0.383440\n",
      "Epoch[780/1000], loss: 0.383193\n",
      "Epoch[800/1000], loss: 0.382955\n",
      "Epoch[820/1000], loss: 0.382722\n",
      "Epoch[840/1000], loss: 0.382493\n",
      "Epoch[860/1000], loss: 0.382267\n",
      "Epoch[880/1000], loss: 0.382043\n",
      "Epoch[900/1000], loss: 0.381821\n",
      "Epoch[920/1000], loss: 0.381600\n",
      "Epoch[940/1000], loss: 0.381380\n",
      "Epoch[960/1000], loss: 0.381161\n",
      "Epoch[980/1000], loss: 0.380942\n",
      "Epoch[1000/1000], loss: 0.380724\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dd2b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 开始训练\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    inputs = Variable(x_train)\n",
    "    target = Variable(y_train)\n",
    "\n",
    "    # forward\n",
    "    out = model(inputs)\n",
    "    loss = criterion(out, target)\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print('Epoch[{}/{}], loss: {:.6f}'\n",
    "              .format(epoch+1, num_epochs, loss.data[0]))\n",
    "\n",
    "model.eval()\n",
    "predict = model(Variable(x_train))\n",
    "predict = predict.data.numpy()\n",
    "plt.plot(x_train.numpy(), y_train.numpy(), 'ro', label='Original data')\n",
    "plt.plot(x_train.numpy(), predict, label='Fitting Line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-11T19:51:17.538194",
     "start_time": "2017-08-11T19:51:17.534852"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model.state_dict(), './linear.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-11T22:50:46.091986",
     "start_time": "2017-08-11T22:50:46.071481"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Variable in module torch.autograd.variable object:\n",
      "\n",
      "class Variable(torch._C._VariableBase)\n",
      " |  Wraps a tensor and records the operations applied to it.\n",
      " |  \n",
      " |  Variable is a thin wrapper around a Tensor object, that also holds\n",
      " |  the gradient w.r.t. to it, and a reference to a function that created it.\n",
      " |  This reference allows retracing the whole chain of operations that\n",
      " |  created the data. If the Variable has been created by the user, its creator\n",
      " |  will be ``None`` and we call such objects *leaf* Variables.\n",
      " |  \n",
      " |  Since autograd only supports scalar valued function differentiation, grad\n",
      " |  size always matches the data size. Also, grad is normally only allocated\n",
      " |  for leaf variables, and will be always zero otherwise.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      data: Wrapped tensor of any type.\n",
      " |      grad: Variable holding the gradient of type and location matching\n",
      " |          the ``.data``.  This attribute is lazily allocated and can't\n",
      " |          be reassigned.\n",
      " |      requires_grad: Boolean indicating whether the Variable has been\n",
      " |          created by a subgraph containing any Variable, that requires it.\n",
      " |          See :ref:`excluding-subgraphs` for more details.\n",
      " |          Can be changed only on leaf Variables.\n",
      " |      volatile: Boolean indicating that the Variable should be used in\n",
      " |          inference mode, i.e. don't save the history. See\n",
      " |          :ref:`excluding-subgraphs` for more details.\n",
      " |          Can be changed only on leaf Variables.\n",
      " |      creator: Function of which the variable was an output. For leaf\n",
      " |          (user created) variables it's ``None``. Read-only attribute.\n",
      " |  \n",
      " |  Parameters:\n",
      " |      data (any tensor class): Tensor to wrap.\n",
      " |      requires_grad (bool): Value of the requires_grad flag. **Keyword only.**\n",
      " |      volatile (bool): Value of the volatile flag. **Keyword only.**\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Variable\n",
      " |      torch._C._VariableBase\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  __deepcopy__(self, memo)\n",
      " |  \n",
      " |  __div__(self, other)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |  \n",
      " |  __ge__(self, other)\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |  \n",
      " |  __gt__(self, other)\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |  \n",
      " |  __iadd__(self, other)\n",
      " |  \n",
      " |  __idiv__(self, other)\n",
      " |  \n",
      " |  __imul__(self, other)\n",
      " |  \n",
      " |  __ipow__(self, other)\n",
      " |  \n",
      " |  __isub__(self, other)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __le__(self, other)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |  \n",
      " |  __matmul__(self, other)\n",
      " |  \n",
      " |  __mod__(self, other)\n",
      " |  \n",
      " |  __mul__(self, other)\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |  \n",
      " |  __neg__(self)\n",
      " |  \n",
      " |  __pow__(self, other)\n",
      " |  \n",
      " |  __radd__ = __add__(self, other)\n",
      " |  \n",
      " |  __rdiv__(self, other)\n",
      " |  \n",
      " |  __reduce_ex__(self, proto)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __rmul__ = __mul__(self, other)\n",
      " |  \n",
      " |  __rpow__(self, other)\n",
      " |  \n",
      " |  __rsub__(self, other)\n",
      " |  \n",
      " |  __rtruediv__ = __rdiv__(self, other)\n",
      " |  \n",
      " |  __setitem__(self, key, value)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sub__(self, other)\n",
      " |  \n",
      " |  __truediv__ = __div__(self, other)\n",
      " |  \n",
      " |  abs(self)\n",
      " |  \n",
      " |  acos(self)\n",
      " |  \n",
      " |  add(self, other)\n",
      " |  \n",
      " |  add_(self, other)\n",
      " |  \n",
      " |  addbmm(self, *args)\n",
      " |  \n",
      " |  addbmm_(self, *args)\n",
      " |  \n",
      " |  addcdiv(self, *args)\n",
      " |  \n",
      " |  addcmul(self, *args)\n",
      " |  \n",
      " |  addmm(self, *args)\n",
      " |  \n",
      " |  addmm_(self, *args)\n",
      " |  \n",
      " |  addmv(self, *args)\n",
      " |  \n",
      " |  addmv_(self, *args)\n",
      " |  \n",
      " |  addr(self, *args)\n",
      " |  \n",
      " |  addr_(self, *args)\n",
      " |  \n",
      " |  asin(self)\n",
      " |  \n",
      " |  atan(self)\n",
      " |  \n",
      " |  backward(self, gradient=None, retain_variables=False)\n",
      " |      Computes the gradient of current variable w.r.t. graph leaves.\n",
      " |      \n",
      " |      The graph is differentiated using the chain rule. If the variable is\n",
      " |      non-scalar (i.e. its data has more than one element) and requires\n",
      " |      gradient, the function additionaly requires specifying ``gradient``.\n",
      " |      It should be a tensor of matching type and location, that contains\n",
      " |      the gradient of the differentiated function w.r.t. ``self``.\n",
      " |      \n",
      " |      This function accumulates gradients in the leaves - you might need to zero\n",
      " |      them before calling it.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          gradient (Tensor): Gradient of the differentiated function\n",
      " |              w.r.t. the data. Required only if the data has more than one\n",
      " |              element. Type and location should match these of ``self.data``.\n",
      " |          retain_variables (bool): If ``True``, buffers necessary for computing\n",
      " |              gradients won't be freed after use. It is only necessary to\n",
      " |              specify ``True`` if you want to differentiate some subgraph multiple\n",
      " |              times (in some cases it will be much more efficient to use\n",
      " |              `autograd.backward`).\n",
      " |  \n",
      " |  baddbmm(self, *args)\n",
      " |  \n",
      " |  baddbmm_(self, *args)\n",
      " |  \n",
      " |  bernoulli(self)\n",
      " |  \n",
      " |  bmm(self, batch)\n",
      " |  \n",
      " |  byte(self)\n",
      " |  \n",
      " |  ceil(self)\n",
      " |  \n",
      " |  char(self)\n",
      " |  \n",
      " |  chunk(self, num_chunks, dim=0)\n",
      " |  \n",
      " |  clamp(self, min=None, max=None)\n",
      " |  \n",
      " |  clone(self)\n",
      " |  \n",
      " |  contiguous(self)\n",
      " |  \n",
      " |  cos(self)\n",
      " |  \n",
      " |  cosh(self)\n",
      " |  \n",
      " |  cpu(self)\n",
      " |  \n",
      " |  cross(self, other, dim=-1)\n",
      " |  \n",
      " |  cuda(self, device_id=None, async=False)\n",
      " |  \n",
      " |  cumsum(self, dim)\n",
      " |  \n",
      " |  detach(self)\n",
      " |      Returns a new Variable, detached from the current graph.\n",
      " |      \n",
      " |      Result will never require gradient. If the input is volatile, the output\n",
      " |      will be volatile too.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |        Returned Variable uses the same data tensor, as the original one, and\n",
      " |        in-place modifications on either of them will be seen, and may trigger\n",
      " |        errors in correctness checks.\n",
      " |  \n",
      " |  detach_(self)\n",
      " |      Detaches the Variable from the graph that created it, making it a leaf.\n",
      " |  \n",
      " |  diag(self, diagonal_idx=0)\n",
      " |  \n",
      " |  dist(self, tensor, p=2)\n",
      " |  \n",
      " |  div(self, other)\n",
      " |  \n",
      " |  div_(self, other)\n",
      " |  \n",
      " |  dot(self, other)\n",
      " |  \n",
      " |  double(self)\n",
      " |  \n",
      " |  eq(self, other)\n",
      " |  \n",
      " |  exp(self)\n",
      " |  \n",
      " |  exp_(self)\n",
      " |  \n",
      " |  expand(self, *sizes)\n",
      " |  \n",
      " |  expand_as(self, tensor)\n",
      " |  \n",
      " |  float(self)\n",
      " |  \n",
      " |  floor(self)\n",
      " |  \n",
      " |  fmod(self, value)\n",
      " |  \n",
      " |  frac(self)\n",
      " |  \n",
      " |  gather(self, dim, index)\n",
      " |  \n",
      " |  ge(self, other)\n",
      " |  \n",
      " |  ger(self, vector)\n",
      " |  \n",
      " |  gt(self, other)\n",
      " |  \n",
      " |  half(self)\n",
      " |  \n",
      " |  index_add(self, dim, index, tensor)\n",
      " |  \n",
      " |  index_add_(self, dim, index, tensor)\n",
      " |  \n",
      " |  index_copy(self, dim, index, tensor)\n",
      " |  \n",
      " |  index_copy_(self, dim, index, tensor)\n",
      " |  \n",
      " |  index_fill(self, dim, index, value)\n",
      " |  \n",
      " |  index_fill_(self, dim, index, value)\n",
      " |  \n",
      " |  index_select(self, dim, index)\n",
      " |  \n",
      " |  int(self)\n",
      " |  \n",
      " |  is_same_size(self, other_var)\n",
      " |  \n",
      " |  kthvalue(self, dim)\n",
      " |  \n",
      " |  le(self, other)\n",
      " |  \n",
      " |  lerp(self, tensor, weight)\n",
      " |  \n",
      " |  log(self)\n",
      " |  \n",
      " |  log1p(self)\n",
      " |  \n",
      " |  long(self)\n",
      " |  \n",
      " |  lt(self, other)\n",
      " |  \n",
      " |  masked_copy(self, mask, variable)\n",
      " |  \n",
      " |  masked_copy_(self, mask, variable)\n",
      " |  \n",
      " |  masked_fill(self, mask, value)\n",
      " |  \n",
      " |  masked_fill_(self, mask, value)\n",
      " |  \n",
      " |  masked_select(self, mask)\n",
      " |  \n",
      " |  max(self, dim=None)\n",
      " |  \n",
      " |  mean(self, dim=None)\n",
      " |  \n",
      " |  median(self, dim)\n",
      " |  \n",
      " |  min(self, dim=None)\n",
      " |  \n",
      " |  mm(self, matrix)\n",
      " |  \n",
      " |  mode(self, dim)\n",
      " |  \n",
      " |  mul(self, other)\n",
      " |  \n",
      " |  mul_(self, other)\n",
      " |  \n",
      " |  multinomial(self, num_samples=1, with_replacement=False)\n",
      " |  \n",
      " |  mv(self, vector)\n",
      " |  \n",
      " |  narrow(self, dim, start_index, length)\n",
      " |  \n",
      " |  ne(self, other)\n",
      " |  \n",
      " |  neg(self)\n",
      " |  \n",
      " |  neg_(self)\n",
      " |  \n",
      " |  norm(self, p=2, dim=None)\n",
      " |  \n",
      " |  permute(self, *permutation)\n",
      " |  \n",
      " |  pow(self, other)\n",
      " |  \n",
      " |  prod(self, dim=None)\n",
      " |  \n",
      " |  reciprocal(self)\n",
      " |  \n",
      " |  register_hook(self, hook)\n",
      " |      Registers a backward hook.\n",
      " |      \n",
      " |      The hook will be called every time a gradient with respect to the\n",
      " |      variable is computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(grad) -> Variable or None\n",
      " |      \n",
      " |      The hook should not modify its argument, but it can optionally return\n",
      " |      a new gradient which will be used in place of :attr:`grad`.\n",
      " |      \n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> v = Variable(torch.Tensor([0, 0, 0]), requires_grad=True)\n",
      " |          >>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n",
      " |          >>> v.backward(torch.Tensor([1, 1, 1]))\n",
      " |          >>> v.grad.data\n",
      " |           2\n",
      " |           2\n",
      " |           2\n",
      " |          [torch.FloatTensor of size 3]\n",
      " |          >>> h.remove()  # removes the hook\n",
      " |  \n",
      " |  reinforce(self, reward)\n",
      " |      Registers a reward obtained as a result of a stochastic process.\n",
      " |      \n",
      " |      Differentiating stochastic nodes requires providing them with reward\n",
      " |      value. If your graph contains any stochastic operations, you should\n",
      " |      call this function on their outputs. Otherwise an error will be raised.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          reward(Tensor): Tensor with per-element rewards. It has to match\n",
      " |              the device location and shape of Variable's data.\n",
      " |  \n",
      " |  remainder(self, value)\n",
      " |  \n",
      " |  renorm(self, p, dim, maxnorm)\n",
      " |  \n",
      " |  repeat(self, *repeats)\n",
      " |  \n",
      " |  resize(self, *sizes)\n",
      " |  \n",
      " |  resize_as(self, variable)\n",
      " |  \n",
      " |  round(self)\n",
      " |  \n",
      " |  rsqrt(self)\n",
      " |  \n",
      " |  scatter(self, dim, index, source)\n",
      " |  \n",
      " |  scatter_(self, dim, index, source)\n",
      " |  \n",
      " |  select(self, dim, _index)\n",
      " |  \n",
      " |  short(self)\n",
      " |  \n",
      " |  sigmoid(self)\n",
      " |  \n",
      " |  sigmoid_(self)\n",
      " |  \n",
      " |  sign(self)\n",
      " |  \n",
      " |  sin(self)\n",
      " |  \n",
      " |  sinh(self)\n",
      " |  \n",
      " |  sort(self, dim=None, descending=False)\n",
      " |  \n",
      " |  split(self, split_size, dim=0)\n",
      " |  \n",
      " |  sqrt(self)\n",
      " |  \n",
      " |  squeeze(self, dim=None)\n",
      " |  \n",
      " |  std(self, dim=None, unbiased=True)\n",
      " |  \n",
      " |  sub(self, other)\n",
      " |  \n",
      " |  sub_(self, other)\n",
      " |  \n",
      " |  sum(self, dim=None)\n",
      " |  \n",
      " |  t(self)\n",
      " |  \n",
      " |  tan(self)\n",
      " |  \n",
      " |  tanh(self)\n",
      " |  \n",
      " |  tanh_(self)\n",
      " |  \n",
      " |  topk(self, k, dim=None, largest=True, sorted=True)\n",
      " |  \n",
      " |  trace(self)\n",
      " |  \n",
      " |  transpose(self, dim1, dim2)\n",
      " |  \n",
      " |  tril(self, diagonal_idx=0)\n",
      " |  \n",
      " |  triu(self, diagonal_idx=0)\n",
      " |  \n",
      " |  trunc(self)\n",
      " |  \n",
      " |  type(self, t)\n",
      " |  \n",
      " |  type_as(self, t)\n",
      " |  \n",
      " |  unsqueeze(self, dim)\n",
      " |  \n",
      " |  var(self, dim=None, unbiased=True)\n",
      " |  \n",
      " |  view(self, *sizes)\n",
      " |  \n",
      " |  view_as(self, tensor)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch._C._VariableBase:\n",
      " |  \n",
      " |  __init__(...)\n",
      " |      x.__init__(...) initializes x; see help(type(x)) for signature\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch._C._VariableBase:\n",
      " |  \n",
      " |  creator\n",
      " |  \n",
      " |  data\n",
      " |  \n",
      " |  grad\n",
      " |  \n",
      " |  output_nr\n",
      " |  \n",
      " |  requires_grad\n",
      " |  \n",
      " |  volatile\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch._C._VariableBase:\n",
      " |  \n",
      " |  __new__ = <built-in method __new__ of type object>\n",
      " |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-11T22:50:34.215496",
     "start_time": "2017-08-11T22:50:34.179063"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.9048e-01 -4.5573e-02 -1.0660e-01  ...  -1.6881e+00  7.5227e-01  1.5410e+00\n",
       " 6.0501e-01  7.0981e-01  6.7301e-02  ...  -1.4636e-01 -3.5438e-02  2.9461e-01\n",
       " 1.1814e+00  1.5317e+00 -2.3029e-01  ...  -1.6250e-01  8.9886e-01  1.8541e-01\n",
       "                ...                   ⋱                   ...                \n",
       "-1.3005e+00 -1.5443e-01 -3.1878e-02  ...   5.4347e-02  1.3440e+00  1.4266e+00\n",
       " 6.0846e-01  4.7625e-01  1.6940e+00  ...   1.5850e-01 -1.3444e+00 -1.1079e-02\n",
       "-1.9834e+00  1.0492e+00  1.2114e+00  ...   5.7051e-03  1.3604e+00  5.9990e-01\n",
       "[torch.FloatTensor of size 64x1000]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-11T22:50:06.876894",
     "start_time": "2017-08-11T22:50:06.872190"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.2270  1.7872 -1.1285  0.4430 -0.5078\n",
       " 0.4790 -0.6506  0.8788 -2.1128  0.0298\n",
       "-0.3644 -0.3018  1.2617  0.4955 -0.3260\n",
       "-0.0446  0.2773 -0.4059  1.3936 -1.2565\n",
       "[torch.FloatTensor of size 4x5]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.randn(4, 5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
